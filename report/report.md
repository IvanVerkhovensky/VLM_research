# VLA for manipulation: grounding языка и OOD-устойчивость — мини-аудит на SmolVLA

Контекст - трек VLA for manipulation: multi-skill, long-horizon, robustness, embodiment-agnostic.  
 я сделал воспроизводимый аудит публичной VLA-политики и использовал его как доказательство цифры/графики для формулировки открытой проблемы и гипотезы решения.

---

## 1) Свежие работы (2022–2025) на которые я опирался

Набор выбран так, чтобы покрыть VLA как класс моделей, масштабирование и перенос, смесь embodiment’ов, и открытые пайплайны.

- RT-2 (2023) — пример VLA в современном смысле -  перенос vision-language представлений (включая web-knowledge) в робот-управление. Важен как ориентир по семантической генерализации и постановке задач.
- Open X-Embodiment / RT-X (2024) — большой шаг к *embodiment-agnostic*: единый формат данных + обучение на смеси роботов. Хорошо показывает, что многие проблемы лежат в данных/распределениях и несовпадении observation/action spaces.
- Octo (2024) — открытая generalist policy; полезна как “реальный baseline” и как пример практической адаптации к новым сенсорам/экшенам.
- OpenVLA (2024) — открытая VLA-модель + инженерные рецепты оценки и дообучения; снижает порог воспроизводимости.
- SmolVLA + LeRobot (2025) — компактная VLA-политика и tooling, которые реально запускаются на consumer hardware (в моём случае  MacBook MPS). Это позволило не ограничиваться чтением текстов, а подкрепить понимание проблемы измерениями.

---

## 2) Открытая проблема 

Даже современные VLA-политики могут быть одновременно:

1) недостаточно привязаны к языку: инструкция влияет на действие, но не всегда предсказуемо — модель может частично ездить на визуальном контексте, а язык становится слабым модификатором;

2) хрупкими к небольшим визуальным OOD-сдвигам: изменения освещения, шум сенсора, частичная окклюзия — это норма в реальном мире, но они могут заметно менять предсказанные действия.

В long-horizon и multi-skill сценариях эти эффекты накапливаются и превращаются в нестабильное выполнение.

---

## 3) Гипотеза преодоления барьера

Гипотеза: если при fine-tuning/post-training добавить два вида регуляризации, то можно одновременно улучшить instruction-following и OOD-устойчивость без кратного роста данных:

- Counterfactual language contrast: для одного наблюдения подать конфликтующие инструкции и штрафовать модель, если действия почти не различаются (форсировать каузальную зависимость от языка).
- Visual consistency regularization: для “допустимых” perturbations (brightness/noise/occlusion в разумных пределах) штрафовать большой action drift (уменьшать хрупкость к OOD).

В рамках дедлайна я не делал полный fine-tune с этими регуляризаторами. Вместо этого я построил и провёл корректный baseline-аудит, который:
- показывает, что проблема измерима на публичном baseline,
- задаёт воспроизводимый протокол “до/после” для проверки гипотезы на следующем шаге.

---

## 4) воспроизведение/измерения

Модель: HuggingFaceVLA/smolvla_libero  
Датасет (LeRobot v3.0): eunyoung927/smol-libero-v30  
Железо: MacBook, Apple MPS

Я сделал два офлайн-аудита:

### 4.1 Language sensitivity audit
Для одного и того же наблюдения сравниваю предсказанные действия при:
- исходной инструкции
- пустой инструкции
- “контрфактуальной” инструкции (из небольшого пула)

Метрика: L2(action_orig - action_variant).

### 4.2 OOD visual sensitivity audit
Для тех же наблюдений вношу perturbations в изображения обеих камер:
- brightness
- gaussian noise
- occlusion

И измеряю дрейф действий относительно исходного.

### 4.3 очереди действий + детерминизм

SmolVLA использует action chunking/внутренние очереди, поэтому повторные вызовы select_action() могут возвращать разные действия даже на одном и том же входе.  
Чтобы сравнение отражало именно влияние входа (язык/картинка), я:
- сбрасываю состояние политики перед каждым предсказанием,
- фиксирую seed “на фрейм” одинаково для orig/variant.

Контроль качества протокола: mean_dist_repeat ≈ 0.

---

## 5) Результаты 

Графики:
- figures/language_sensitivity.png
- figures/ood_sensitivity.png

JSON:
- outputs/language_seed*.json
- outputs/ood_*_seed*.json

### 5.1 Язык (пример: 120 фреймов, seed=0)
- repeat drift: 0.0
- mean drift (empty instruction): ~0.779
- mean drift (counterfactual/shuffle): ~0.704

(Смысл: язык влияет на предсказания измеримо; при этом протокол детерминирован, то есть. это не “шум”.)

### 5.2 OOD JSON + график
Визуальные perturbations (яркость/шум/окклюзия) дают измеримый action drift. Это прокси метрика (не success-rate), но она хорошо показывает наличие сигнала хрупкости к OOD.

---

## 6) Что бы я сделал следующим шагом 
1) Success-rate eval в симуляции (LIBERO или аналог) + controlled perturbations, чтобы связать action drift с реальным успехом/провалом задач.
2) Лёгкий fine-tune (например LoRA) baseline vs +регуляризаторы из гипотезы и сравнение “до/после” по тем же аудитам и по success-rate.
3) Отчёт mean±std по нескольким seed’ам - часть уже сделана в моем репозитории.
